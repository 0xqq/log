ingest.sources = from_kafka_src
ingest.channels = to_avro_channel
ingest.sinks = to_avro_sink

ingest.sources.from_kafka_src.type = org.apache.flume.source.kafka.KafkaSource
ingest.sources.from_kafka_src.kafka.bootstrap.servers = kafka1:6667,kafka2:6667,kafka3:6667
ingest.sources.from_kafka_src.kafka.topics.regex = ^topic[0-9]$
#ingest.sources.from_kafka_src.kafka.topics = mytopic
ingest.sources.from_kafka_src.kafka.batchSize = 10000
ingest.sources.from_kafka_src.channels = to_avro_channel
ingest.sources.from_kafka_src.kafka.consumer.group.id = flume-consumer
ingest.sources.from_kafka_src.kafka.consumer.timeout.ms = 6000
ingest.sources.from_kafka_src.kafka.zookeeper.session.timeout.ms=6000
ingest.sources.from_kafka_src.kafka.rebalance.max.retries=5
ingest.sources.from_kafka_src.kafka.rebalance.backoff.ms=1500
#ingest.sources.from_kafka_src.kafka.consumer.security.protocol = SASL_PLAINTEXT
#ingest.sources.from_kafka_src.kafka.consumer.sasl.mechanism = GSSAPI
#ingest.sources.from_kafka_src.kafka.consumer.sasl.kerberos.service.name = kafka

#ingest.channels = c1 c2 c3
#ingest.sources.from_kafka_src.selector.type = multiplexing
#ingest.sources.from_kafka_src.selector.header = xxxName
#ingest.sources.from_kafka_src.selector.mapping.value1 = c1
#ingest.sources.from_kafka_src.selector.mapping.value2 = c1 c2
#ingest.sources.from_kafka_src.selector.mapping.value3 = c3

#ingest.channels = c1 c2 c3
#ingest.sources.from_kafka_src.selector.type = replicating
#ingest.sources.from_kafka_src.channels = c1 c2 c3
#ingest.sources.from_kafka_src.selector.optional = c3

#ingest.channels.to_avro_channel.type = file
#ingest.channels.to_avro_channel.checkpointDir = /tmp/flume/to_avro_sink/checkpoint
#ingest.channels.to_avro_channel.dataDirs = /tmp/flume/to_avro_sink/data
#ingest.channels.to_avro_channel.transactionCapacity = 10000
#ingest.channels.to_avro_channel.capacity = 1000000

ingest.channels.to_avro_channel.type = memory
ingest.channels.to_avro_channel.capacity = 100000
ingest.channels.to_avro_channel.transactionCapacity = 100000


#ingest.sinkgroups = g1
#ingest.sinkgroups.g1.sinks = k1 k2 k3
#ingest.sinkgroups.g1.processor.type = load_balance
#ingest.sinkgroups.g1.processor.backoff = true
#ingest.sinkgroups.g1.processor.selector = random
##ingest.sinkgroups.g1.processor.selector = round_robin

#ingest.sinkgroups = g1
#ingest.sinkgroups.g1.sinks = k1 k2 k3
#ingest.sinkgroups.g1.processor.type = failover
#ingest.sinkgroups.g1.processor.priority.k1 = 5
#ingest.sinkgroups.g1.processor.priority.k2 = 10
#ingest.sinkgroups.g1.processor.priority.k3 = 15
#ingest.sinkgroups.g1.processor.maxpenalty = 10000

ingest.sinks.to_avro_sink.type = hdfs
ingest.sinks.to_avro_sink.hdfs.path = /user/flume/task/avro/%Y/%n/%e/%k
ingest.sinks.to_avro_sink.hdfs.filePrefix = flume_task_avro
ingest.sinks.to_avro_sink.hdfs.fileSuffix = .avro
ingest.sinks.to_avro_sink.hdfs.rollCount = 100
ingest.sinks.to_avro_sink.hdfs.rollInterval = 0
ingest.sinks.to_avro_sink.hdfs.rollSize = 0
ingest.sinks.to_avro_sink.hdfs.useLocalTimeStamp = true
ingest.sinks.to_avro_sink.hdfs.timeZone = UTC
ingest.sinks.to_avro_sink.hdfs.writeFormat = Text
ingest.sinks.to_avro_sink.hdfs.fileType = DataStream
ingest.sinks.to_avro_sink.serializer = avro_event
ingest.sinks.to_avro_sink.channel = to_avro_channel