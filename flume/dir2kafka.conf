ingest.sources = from_dir_src
ingest.channels = to_kafka_channel
ingest.sinks = to_kafka_sink

ingest.sources.from_dir_src.type = spooldir
ingest.sources.from_dir_src.channels = to_kafka_channel
ingest.sources.from_dir_src.spoolDir = /data/logs/nginx
ingest.sources.from_dir_src.fileHeader = true
ingest.sources.from_dir_src.recursiveDirectorySearch = true
ingest.sources.from_dir_src.basenameHeader = true
ingest.sources.from_dir_src.basenameHeaderKey = basename
ingest.sources.from_dir_src.parentDirectoryHeader = true
ingest.sources.from_dir_src.parentDirectoryHeaderKey = parentDirectory
ingest.sources.from_dir_src.relativeParentDirectoryHeader = true
ingest.sources.from_dir_src.relativeParentDirectoryHeaderKey = relativeParentDirectory
#ingest.sources.from_dir_src.deserializer = org.apache.flume.sink.solr.morphline.BlobDeserializer$Builder

#ingest.sources = from_dir_src
#ingest.sources.from_dir_src.type = TAILDIR
#ingest.sources.from_dir_src.positionFile = /var/log/flume/taildir_position.json
#ingest.sources.from_dir_src.filegroups = f1 f2
#ingest.sources.from_dir_src.filegroups.f1 = /data/logs/nginx/*_access.log
#ingest.sources.from_dir_src.headers.f1.headerKey1 = nginx
#ingest.sources.from_dir_src.headers.f1.headerKey2 = access
#ingest.sources.from_dir_src.filegroups.f2 = /var/log/nginx/*_error.log
#ingest.sources.from_dir_src.headers.f2.headerKey1 = nginx
#ingest.sources.from_dir_src.headers.f2.headerKey2 = error
#ingest.sources.from_dir_src.fileHeader = true


#ingest.channels.to_kafka_channel.type = file
#ingest.channels.to_kafka_channel.checkpointDir = /tmp/flume/to_avro_sink/checkpoint
#ingest.channels.to_kafka_channel.dataDirs = /tmp/flume/to_avro_sink/data
#ingest.channels.to_kafka_channel.transactionCapacity = 10000
#ingest.channels.to_kafka_channel.capacity = 1000000

#ingest.channels = c1 c2 c3
#ingest.sources.from_kafka_src.selector.type = multiplexing
#ingest.sources.from_kafka_src.selector.header = xxxName
#ingest.sources.from_kafka_src.selector.mapping.value1 = c1
#ingest.sources.from_kafka_src.selector.mapping.value2 = c1 c2
#ingest.sources.from_kafka_src.selector.mapping.value3 = c3

#ingest.channels = c1 c2 c3
#ingest.sources.from_kafka_src.selector.type = replicating
#ingest.sources.from_kafka_src.channels = c1 c2 c3
#ingest.sources.from_kafka_src.selector.optional = c3

ingest.channels.to_kafka_channel.type = memory
ingest.channels.to_kafka_channel.capacity = 100000
ingest.channels.to_kafka_channel.transactionCapacity = 100000


#ingest.sinkgroups = g1
#ingest.sinkgroups.g1.sinks = k1 k2 k3
#ingest.sinkgroups.g1.processor.type = load_balance
#ingest.sinkgroups.g1.processor.backoff = true
#ingest.sinkgroups.g1.processor.selector = random
##ingest.sinkgroups.g1.processor.selector = round_robin

#ingest.sinkgroups = g1
#ingest.sinkgroups.g1.sinks = k1 k2 k3
#ingest.sinkgroups.g1.processor.type = failover
#ingest.sinkgroups.g1.processor.priority.k1 = 5
#ingest.sinkgroups.g1.processor.priority.k2 = 10
#ingest.sinkgroups.g1.processor.priority.k3 = 15
#ingest.sinkgroups.g1.processor.maxpenalty = 10000


ingest.sinks.to_kafka_sink.channel = to_kafka_channel
ingest.sinks.to_kafka_sink.type = org.apache.flume.sink.kafka.KafkaSink
ingest.sinks.to_kafka_sink.kafka.topic = mytopic
#ingest.sinks.to_kafka_sink.kafka.topic = %{basename}
ingest.sinks.to_kafka_sink.kafka.bootstrap.servers = kafkingest:6667,kafka2:6667,kafka3:6667
ingest.sinks.to_kafka_sink.kafka.flumeBatchSize = 2000
ingest.sinks.to_kafka_sink.kafka.producer.acks = 1
ingest.sinks.to_kafka_sink.kafka.producer.linger.ms = 1
ingest.sinks.to_kafka_sink.kafka.producer.compression.type = snappy
ingest.sinks.to_kafka_sink.kafka.kafka.producer.type = async
ingest.sinks.to_kafka_sink.kafka.kafka.encoding = UTF-8
ingest.sinks.to_kafka_sink.kafka.kafka.partitioner.class = com.hisun.flume.sink.kafka.partition.RandomPartioner
#ingest.sinks.to_kafka_sink.kafka.producer.security.protocol = SASL_PLAINTEXT
#ingest.sinks.to_kafka_sink.kafka.producer.sasl.mechanism = GSSAPI
#ingest.sinks.to_kafka_sink.kafka.producer.sasl.kerberos.service.name = kafka