ingest.sources = from_kafka_src
ingest.channels = to_hive_channel
ingest.sinks = to_hive_sink

ingest.sources.from_kafka_src.type = org.apache.flume.source.kafka.KafkaSource
ingest.sources.from_kafka_src.kafka.bootstrap.servers = kafka1:6667,kafka2:6667,kafka3:6667
ingest.sources.from_kafka_src.kafka.topics.regex = ^topic[0-9]$
#ingest.sources.from_kafka_src.kafka.topics = mytopic
ingest.sources.from_kafka_src.channels = to_avro_channel
ingest.sources.from_kafka_src.kafka.consumer.group.id = flume-consumer
#ingest.sources.from_kafka_src.kafka.consumer.security.protocol = SASL_PLAINTEXT
#ingest.sources.from_kafka_src.kafka.consumer.sasl.mechanism = GSSAPI
#ingest.sources.from_kafka_src.kafka.consumer.sasl.kerberos.service.name = kafka

#ingest.sources.from_avro_src.type = avro
#ingest.sources.from_avro_src.channels = to_avro_channel
#ingest.sources.from_avro_src.bind = 0.0.0.0
#ingest.sources.from_avro_src.port = 4141


#ingest.channels = c1 c2 c3
#ingest.sources.from_kafka_src.selector.type = multiplexing
#ingest.sources.from_kafka_src.selector.header = xxxName
#ingest.sources.from_kafka_src.selector.mapping.value1 = c1
#ingest.sources.from_kafka_src.selector.mapping.value2 = c1 c2
#ingest.sources.from_kafka_src.selector.mapping.value3 = c3

#ingest.channels = c1 c2 c3
#ingest.sources.from_kafka_src.selector.type = replicating
#ingest.sources.from_kafka_src.channels = c1 c2 c3
#ingest.sources.from_kafka_src.selector.optional = c3

#ingest.channels.to_avro_channel.type = file
#ingest.channels.to_avro_channel.checkpointDir = /tmp/flume/to_avro_sink/checkpoint
#ingest.channels.to_avro_channel.dataDirs = /tmp/flume/to_avro_sink/data
#ingest.channels.to_avro_channel.transactionCapacity = 10000
#ingest.channels.to_avro_channel.capacity = 1000000

ingest.channels.to_avro_channel.type = memory
ingest.channels.to_avro_channel.capacity = 100000
ingest.channels.to_avro_channel.transactionCapacity = 100000


#ingest.sinkgroups = g1
#ingest.sinkgroups.g1.sinks = k1 k2 k3
#ingest.sinkgroups.g1.processor.type = load_balance
#ingest.sinkgroups.g1.processor.backoff = true
#ingest.sinkgroups.g1.processor.selector = random
##ingest.sinkgroups.g1.processor.selector = round_robin

#ingest.sinkgroups = g1
#ingest.sinkgroups.g1.sinks = k1 k2 k3
#ingest.sinkgroups.g1.processor.type = failover
#ingest.sinkgroups.g1.processor.priority.k1 = 5
#ingest.sinkgroups.g1.processor.priority.k2 = 10
#ingest.sinkgroups.g1.processor.priority.k3 = 15
#ingest.sinkgroups.g1.processor.maxpenalty = 10000

ingest.sinks.to_hive_sink.type = hive
ingest.sinks.to_hive_sink.hive.metastore = thrift://hive.meizu.mz:9083
ingest.sinks.to_hive_sink.hive.database = default
ingest.sinks.to_hive_sink.hive.table = test
ingest.sinks.to_hive_sink.hive.partition = asia,%{country},%y-%m-%d-%H-%M
ingest.sinks.to_hive_sink.batchSize = 10000
ingest.sinks.to_hive_sink.timeZone = UTC
ingest.sinks.to_hive_sink.useLocalTimeStamp = false
ingest.sinks.to_hive_sink.round = true
ingest.sinks.to_hive_sink.roundUnit = second
ingest.sinks.to_hive_sink.roundValue=1
ingest.sinks.to_hive_sink.serializer = DELIMITED
ingest.sinks.to_hive_sink.serializer.delimiter = ,
ingest.sinks.to_hive_sink.serializer.serdeSeparator = '\t'
ingest.sinks.to_hive_sink.serializer.fieldnames = userid,notify,targetid,timestamp
ingest.sinks.to_hive_sink.channel = to_hive_channel